''' Data ingestion config is required first, which tells us where the data needs to be stored, all these things
, basic info where is training path, testing etc
'''

import pymongo.mongo_client
from Loan_prediction.exception.exception import custom_exception
from Loan_prediction.logger.logger import logger
from Loan_prediction.entity.config_entity import DataIngestionConfig
from Loan_prediction.entity.artifact_entity import DataIngestionArtifact

import os
import sys
import pymongo
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from dotenv import load_dotenv # dotenv is used to extract url from .env file
load_dotenv()

MONGO_DB_URL = os.getenv("MONGODB_URL")

class DataIngestion:
    def __init__(self,config : DataIngestionConfig):    # constructor, runs automatically when you create a new instance of class
        try:
            self.config = config #saving the incoming variable so the whole class can use it

        except Exception as e:
            raise custom_exception(e,sys)
    
    def export_collection_as_df(self):
        try:
            db_name = self.config.database_name
            collection_name = self.config.collection_name

            print(f"Database Name: {db_name},Collection Name :{collection_name}")

            self.mongo_client = pymongo.MongoClient(MONGO_DB_URL) # the handshake - connecting to server
            collection = self.mongo_client[db_name][collection_name] # find specific db then specific collection inside it

            df = pd.DataFrame(list(collection.find()))  # retrive all documents using find and convert them into dataframes

            print(df.head())
            # it removes default unique ID column generated by mongoDB, as it is not needed for analysis
            if "_id" in df.columns.to_list():
                df.drop(columns=["_id"], axis=1, inplace=True)  
            # replace string "na" with actual Nan of numpy
            df.replace({"na" : np.nan}, inplace=True)
            return df


        except Exception as e:
            raise custom_exception(e,sys)
        
    ''' saves data locally in my system, kind of snapshot of data or backup
    reading from local csv is faster, if data changes in mongoDB you have freezed data for your working,
    and if pipeline crashes no need to download it again'''

    def export_data_to_feature_store(self, df:pd.DataFrame):
        try:
            feature_store_file_path = self.config.feature_store_file_path

            # create directory for it first
            dir_path = os.path.dirname(feature_store_file_path)
            os.makedirs(dir_path, exist_ok=True)

            df.to_csv(feature_store_file_path,index=False,header=True) # header = name of columns

        except Exception as e:
            raise custom_exception(e,sys)
        
    
    def split_data_as_train_test(self,df: pd.DataFrame):
        try:
            train_set, test_set = train_test_split(
                df, test_size=self.config.train_test_split_ratio
            )
            logger.info("performed train test split on dataframe")

            logger.info(
                "Exited split_data_as_train_test method of Data_Ingestion class"
            )
            # creating directory first
            dir_path = os.path.dirname(self.config.training_file_path)
            os.makedirs(dir_path, exist_ok=True)

            logger.info("Exporting train and test file path")

            train_set.to_csv(
                self.config.training_file_path, index = False, header =True # header = true means column names are included
            )

            test_set.to_csv(
                self.config.testing_file_path, index = False, header=True
            )

            logger.info("Exported train and test file path")

        except Exception as e:
            raise custom_exception(e,sys)


    def inititiate_data_ingestion(self):
        try:
            df = self.export_collection_as_df() # take data from mongoDB
            self.export_data_to_feature_store(df)   # convert it to dataframe
            self.split_data_as_train_test(df)   # split it and save in artifacts

            dataingestionartifact = DataIngestionArtifact(
                trained_file_path=self.config.training_file_path, 
                test_file_path=self.config.testing_file_path
                )
            
            return dataingestionartifact


        except Exception as e:
            raise custom_exception(e,sys)